{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f522e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51f29c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244e60de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000, Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "data_dir = Path(\"hands-on/data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2470, 0.2435, 0.2616)\n",
    "normalize = transforms.Normalize(mean=cifar_mean, std=cifar_std)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=train_transforms)\n",
    "test_ds = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "pin_memory = device == \"cuda\"\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96cbabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size = 32, patch_size = 4, in_channels = 3, embed_dim = 128 ): # \n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride= patch_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.proj(x) # (B, embed_dim, img_size/patch_size, img_size/patch_size )\n",
    "        x = x.flatten(2).transpose(1,2) # (B, num_patches, embed_dim)\n",
    "        return x \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim,  mlp_ratio=4.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(embed_dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden)\n",
    "        self.act = nn.GELU() # Gaussian Error Linear Units - Empfehlung für alle Transformer \n",
    "        self.fc2 = nn.Linear(hidden,embed_dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, x ):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim) # Normalisierung über alle Feature-Dimensione (nicht batchweit)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop, batch_first=True)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP(embed_dim, mlp_ratio, drop) \n",
    "    \n",
    "    def forward(self,x):\n",
    "        norm1 = self.norm1(x)\n",
    "        attn_out, _ = self.attn(norm1,norm1,norm1)\n",
    "        x = x + self.drop(attn_out) # unterer Teil\n",
    "        norm2 = self.norm2(x)\n",
    "        x = x + self.drop(self.mlp(norm2)) # oberer Teil\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size = 32, patch_size = 4, in_channels = 3, num_classes = 10, embed_dim = 128, depth = 6,\n",
    "                 num_heads = 4,\n",
    "                 mlp_ratio = 4.0,\n",
    "                 drop = 0.1,\n",
    "                 attn_drop = 0.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.rand(1,num_patches+1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.transformer = nn.ModuleList(\n",
    "            [\n",
    "                Block(embed_dim, num_heads, mlp_ratio, drop=drop, attn_drop=attn_drop) for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x),dim=1) \n",
    "        x = x + self.pos_embed # Embedding + Positional Embedding \n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.transformer:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:,0]\n",
    "        return self.head(cls_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc597cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 1.21M\n"
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(img_size=32, patch_size=4, num_classes=10, embed_dim=128, depth=6, num_heads=4, drop=0.1)\n",
    "model.to(torch.device(device))\n",
    "\n",
    "lr = 3e-4\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model params: {num_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97871f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient Clipping \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return running_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85a2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, device)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} | train loss {train_loss:.4f} acc {train_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
