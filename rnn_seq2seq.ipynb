{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "829b5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "path = \"./deu.txt\"\n",
    "\n",
    "lines = open(path, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "lines = lines[:20000]\n",
    "\n",
    "pairs = [ln.split(\"\\t\")[:2] for ln in lines] \n",
    "src_texts, tgt_texts = zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f27411b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_texts[:5], tgt_texts[:5]\n",
    "PAD, UNK, BOS, EOS = 0, 1, 2, 3 # Spezialtokens \n",
    "# PAD = Padding, UNK = Unknown, \n",
    "# BOS = Begin of Sequence, EOS = End of Sequence\n",
    "VOCAB_SIZE = 20004\n",
    "def tokenize(s): return re.findall(r\"\\b\\w+\\b\", s.lower())\n",
    "def build_vocab(texts, max_tokens=VOCAB_SIZE):\n",
    "    from collections import Counter\n",
    "    freq = Counter(tok for t in texts for tok in tokenize(t))\n",
    "    itos = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"] + [w for w,_ in freq.most_common(max_tokens-4)]\n",
    "    return {w:i for i,w in enumerate(itos)}, itos\n",
    "src_texts_vocab, src_itos = build_vocab(src_texts)\n",
    "tgt_texts_vocab, tgt_itos = build_vocab(tgt_texts)\n",
    "\n",
    "src_t = \"find a job\"\n",
    "tgt_t = \"Grüß Gott, Tom\"\n",
    "#src_t = (\"<bos>\",1,2,3,4,\"<pad>\",\"<pad>\",\"<eos>\") \n",
    "#tgt_t = (\"<bos>\",5,3,6,2,3,4, \"<eos>\")\n",
    "\n",
    "\n",
    "def vectorize(text, stoi, max_len, add_bos_eos=False):\n",
    "    ids = [stoi.get(tok, UNK) for tok in tokenize(text)]\n",
    "    if add_bos_eos: ids = [BOS] + ids + [EOS]\n",
    "    ids = ids[:max_len]\n",
    "    if len(ids) < max_len: ids += [PAD]*(max_len-len(ids))\n",
    "    return ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_src, max_tgt = 30, 30  # Maximale Länge \n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src = torch.tensor([vectorize(t, src_texts_vocab, max_src) for t in src_batch])\n",
    "    tgt = torch.tensor([vectorize(t, tgt_texts_vocab, max_tgt, add_bos_eos=True) for t in tgt_batch])\n",
    "    tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "dataset = list(zip(src_texts, tgt_texts))\n",
    "loader = DataLoader(dataset, batch_size= 64, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fae447",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim, hid_dim = 128, 256\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab, emb_dim , padding_idx=PAD)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim , batch_first=True) # [B, S, E] \n",
    "# für das Mini-Batch Training [B, S, E]\n",
    "# B ... Anzahl der Sätze im Batch \n",
    "# S ... Länge der Sequenz\n",
    "# E ... Embedding-Dimension \n",
    "# ohne batch_first [S, B, E]\n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        return hidden # Hidden State \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab, emb_dim, padding_idx=PAD)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=1 , batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim, vocab) # \n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.rnn(x,h) \n",
    "        return self.fc(out), h # logits, hidden_state\n",
    " \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec \n",
    "    \n",
    "    def forward(self, src, tgt_in_dec):\n",
    "    # src ... englische Sätze, \n",
    "    # tgt_in_dec ... aktuelle deutsche Sätze als Eingabe für den Decoder \n",
    "        hidden_enc = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in_dec, hidden_enc) \n",
    "        # Hidden State aus dem Encoder\n",
    "        return logits \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35984737",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"I am a boy.\"\t\n",
    "target = \"Ich bin ein Junge.\"\n",
    "\n",
    "# ----------\n",
    "\n",
    "# Encoder [ \"I am a boy.\" ] -> h\n",
    "# Next Token Prediction \n",
    "# 1. Schritt Decoder [ h, <bos> ] -> \"ich\"\n",
    "# 2. Schritt Decoder [ h, (<bos>,\"ich\")] -> \"bin\" \n",
    "# 3. Schritt Decoder [ h, (<bos>,\"ich\", \"bin\")] -> \"ein\"\n",
    "# 4. Schritt Decoder [ h, (<bos>,\"ich\", \"bin\", \"ein\")] -> \"Junge\"\n",
    "# 5. Schritt Decoder [ h, (<bos>,\"ich\", \"bin\", \"ein\", \"Junge\")] -> \"<eos>\"\n",
    "\n",
    "# X = (h, (<bos>,\"ich\", \"bin\", \"ein\", \"Junge\")) -> Eingabe für Decoder \n",
    "# y = (\"ich\", \"bin\", \"ein\", \"Junge\", \"<eos>\") -> Zielvorgaben für den Loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c42c6b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m src, tgt_in, tgt_out \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m     32\u001b[39m     src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     34\u001b[39m     loss = crit(logits.reshape(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), tgt_out.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m     35\u001b[39m     opt.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mSeq2Seq.forward\u001b[39m\u001b[34m(self, src, tgt_in_dec)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt_in_dec):\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# src ... englische Sätze, \u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# tgt_in_dec ... aktuelle deutsche Sätze als Eingabe für den Decoder \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     hidden_enc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     logits, _ = \u001b[38;5;28mself\u001b[39m.dec(tgt_in_dec, hidden_enc) \n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Hidden State aus dem Encoder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m     14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     _, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1394\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1392\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1406\u001b[39m     result = _VF.gru(\n\u001b[32m   1407\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1408\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1415\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1416\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = Seq2Seq(\n",
    "    Encoder(len(src_texts_vocab), emb_dim=emb_dim, hid_dim=hid_dim),\n",
    "    Decoder(len(tgt_texts_vocab), emb_dim=emb_dim, hid_dim=hid_dim),\n",
    "                ).to(device)\n",
    "\n",
    "crit = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "epochs = 12\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(prompt, max_len=max_tgt):\n",
    "    model.eval()\n",
    "    src = torch.tensor([vectorize(prompt, src_texts_vocab, max_src)], device=device)\n",
    "    h = model.enc(src)\n",
    "    ys = torch.tensor([[BOS]], device=device)\n",
    "    out_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        logits, h = model.dec(ys, h)\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        if next_id in (EOS, PAD): break\n",
    "        out_tokens.append(next_id)\n",
    "        ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "    return \" \".join(tgt_itos[t] for t in out_tokens)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "    for src, tgt_in, tgt_out in loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        logits = model(src, tgt_in) \n",
    "        loss = crit(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient-Clipping -> falls Gradient explodiert \n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"epoch {epoch+1}: loss {running_loss/len(loader):.4f}\")\n",
    "    print(translate(\"I will do my best.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a43e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_0 -> Zufällig\n",
    "#[145,\n",
    "# 10,\n",
    "# 170,\n",
    "# 0,\n",
    "# 0,\n",
    "# 0,...,\n",
    "# \n",
    "# ]\n",
    "\n",
    "# h_1 = Wh * h_0 + Wx*145 \n",
    "# h_2 = .... \n",
    "\n",
    "\n",
    "# RNN1: h_t = Wh*h_{t-1} + Wx*x_t \n",
    "# RNN2: h_t = Wh*h_{t-1} + Wx*x_t  \n",
    "\n",
    "\n",
    "\n",
    "# RNN2(RNN1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f4bd1",
   "metadata": {},
   "source": [
    "# RNN-Encoder-Decoder with Attention (ab 2014 - 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 3.8980\n",
      "ich habe\n",
      "epoch 2: loss 2.6728\n",
      "ich habe\n",
      "epoch 3: loss 2.0711\n",
      "ich habe\n",
      "epoch 4: loss 1.6220\n",
      "ich bin\n",
      "epoch 5: loss 1.2892\n",
      "ich habe\n",
      "epoch 6: loss 1.0363\n",
      "ich habe\n",
      "epoch 7: loss 0.8505\n",
      "ich habe geschummelt\n",
      "epoch 8: loss 0.7165\n",
      "ich habe\n",
      "epoch 9: loss 0.6195\n",
      "ich rastete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m loss = crit(logits.reshape(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), tgt_out.reshape(-\u001b[32m1\u001b[39m))\n\u001b[32m    102\u001b[39m opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m) \u001b[38;5;66;03m# Gradient-Clipping -> falls Gradient explodiert \u001b[39;00m\n\u001b[32m    105\u001b[39m opt.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gits/seminars/it-schulungen.com/PyTorch_RNN_GAN_Transformer/src_live/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def translate(prompt, max_len=max_tgt):\n",
    "    model.eval()\n",
    "    src = torch.tensor([vectorize(prompt, src_texts_vocab, max_src)], device=device)\n",
    "    enc_out, h = model.enc(src)\n",
    "    ys = torch.tensor([[BOS]], device=device)\n",
    "    out_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        logits, h = model.dec(ys, h, enc_out)\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        if next_id in (EOS, PAD): break\n",
    "        out_tokens.append(next_id)\n",
    "        ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "    return \" \".join(tgt_itos[t] for t in out_tokens)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab, emb_dim , padding_idx=PAD)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim , batch_first=True) # [B, S, E] \n",
    "    \n",
    "    def forward(self, x):\n",
    "        token_idx = (x != PAD).long() # 1 = Token, 0 = Pad\n",
    "        x = self.embedding(x)\n",
    "        enc_out, h = self.rnn(x)\n",
    "        return enc_out,h\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(hid_dim, attn_dim, bias=False) # Q\n",
    "        self.W_k = nn.Linear(hid_dim, attn_dim, bias=False) # K \n",
    "        self.W_v = nn.Linear(hid_dim, attn_dim, bias=False) # V\n",
    "        self.scale = attn_dim ** 0.5\n",
    "    \n",
    "    def forward(self, enc_out, h):\n",
    "        q = self.W_q(h.transpose(0,1)) # [B, 1, D ]\n",
    "        #print(q.shape)\n",
    "        k = self.W_k(enc_out)   # [B ,S , D ]\n",
    "        #print(k.shape)\n",
    "        v = self.W_v(enc_out)  # [B ,S , D ]\n",
    "        #print(v.shape)\n",
    "        #print(k.T.shape)\n",
    "        #print(k.transpose(1,2)) # [B, D, 1]\n",
    "        scores = q @ k.transpose(1,2) / self.scale # [B, 1 , S]\n",
    "\n",
    "        attn = torch.softmax(scores, dim=1)\n",
    "        ctx = attn @ v\n",
    "\n",
    "        return ctx\n",
    "\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim, hid_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, emb_dim, padding_idx=PAD)\n",
    "        self.attn = Attention(hid_dim=hid_dim, attn_dim=attn_dim)\n",
    "        self.rnn = nn.RNN(emb_dim + attn_dim , hid_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim,vocab) \n",
    "\n",
    "    def forward(self, x , h, enc_out):\n",
    "        emb = self.emb(x)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(emb.size(1)):\n",
    "            ctx = self.attn(enc_out,h)\n",
    "            rnn_in = torch.cat([emb[:, t:t+1, :], ctx], dim=-1) \n",
    "            # Kombination aktuelle Decoder-Embedding + Attention Context\n",
    "            out, h = self.rnn(rnn_in, h)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        out = torch.cat(outputs, dim=1)\n",
    "        return self.fc(out), h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.enc, self.dec = enc, dec\n",
    "    def forward(self, src, tgt_in):\n",
    "        enc_out, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h, enc_out)\n",
    "        return logits\n",
    " \n",
    "\n",
    "model = Seq2Seq(\n",
    "    Encoder(len(src_texts_vocab), emb_dim=emb_dim, hid_dim=hid_dim),\n",
    "    AttentionDecoder(len(tgt_texts_vocab), emb_dim=emb_dim, hid_dim=hid_dim, attn_dim=hid_dim),\n",
    "                ).to(device)\n",
    "crit = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0 \n",
    "    for src, tgt_in, tgt_out in loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        logits = model(src, tgt_in) \n",
    "        loss = crit(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient-Clipping -> falls Gradient explodiert \n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"epoch {epoch+1}: loss {running_loss/len(loader):.4f}\")\n",
    "    print(translate(\"I rested\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
