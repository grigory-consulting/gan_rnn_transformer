{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \n",
    "# Autoregressive Generierung \n",
    "# Next Token Prediction \n",
    "# 1. Schritt Decoder [ <bos> ] -> \"ich\"\n",
    "# 2. Schritt Decoder [ (<bos>,\"ich\")] -> \"bin\" \n",
    "# 3. Schritt Decoder [ (<bos>,\"ich\", \"bin\")] -> \"ein\"\n",
    "# 4. Schritt Decoder [ (<bos>,\"ich\", \"bin\", \"ein\")] -> \"Junge\"\n",
    "# 5. Schritt Decoder [ (<bos>,\"ich\", \"bin\", \"ein\", \"Junge\")] -> \"<eos>\"\n",
    "# Masked Language Modelling\n",
    "# BERT (Encoder-Only)\n",
    "# Ich gehe in die ________ und trinke Bier. \n",
    "# [\"<bos> , \"Ich\", ....., <mask>, \"und\", .... \"<eos\"] -> \"Kneipe\"\n",
    "\n",
    "# (<bos>,\"ich\", \"bin\", \"ein\", <mask>) -> \"Junge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4693eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0 # Wir spalten die interne Darstellung \n",
    "                                                  # und dann wieder am Ende vereinigen \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch_size, T = Sequenzlänge, C = n_embd \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd, dim=2)\n",
    "        # n_head = nh, C = nh * hs, hs ist Head Size \n",
    "        # C= 768 , n_head = 12, hs = 64 \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q,k,v, is_causal=True) # flash attention / Effiziente Variante \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Die einzelnen Teilheads wieder vereinigen \n",
    "        # Ausgangsprojektion\n",
    "        y = self.c_proj(y) \n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4* config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config) # Causal Self-Attention = Masked Self-Attention \n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x)) # PreNorm \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024      # Maximale Anzahl der Positionen = Länge vom Kontextfenster \n",
    "    vocab_size: int = 50257     # Anzahl der Tokens (gpt2-Tokenizer)\n",
    "    n_layer: int = 12           # Anzahl der Transformerblöcken \n",
    "    n_head: int = 12            # \n",
    "    n_embd: int = 768 \n",
    "\n",
    "\n",
    "class GPT(nn.Module): # Generative Pretrained Transformer 2 \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # Kontextfenster \n",
    "            h = nn.Module(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx = (B, T) B ist batch_size, T ist Sequenzlänge\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size \n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) #   (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # FC \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cee5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
