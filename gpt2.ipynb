{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a9f81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \n",
    "# Autoregressive Generierung \n",
    "# Next Token Prediction \n",
    "# 1. Schritt Decoder [ <bos> ] -> \"ich\"\n",
    "# 2. Schritt Decoder [ (<bos>,\"ich\")] -> \"bin\" \n",
    "# 3. Schritt Decoder [ (<bos>,\"ich\", \"bin\")] -> \"ein\"\n",
    "# 4. Schritt Decoder [ (<bos>,\"ich\", \"bin\", \"ein\")] -> \"Junge\"\n",
    "# 5. Schritt Decoder [ (<bos>,\"ich\", \"bin\", \"ein\", \"Junge\")] -> \"<eos>\"\n",
    "# Masked Language Modelling\n",
    "# BERT (Encoder-Only)\n",
    "# Ich gehe in die ________ und trinke Bier. \n",
    "# [\"<bos> , \"Ich\", ....., <mask>, \"und\", .... \"<eos\"] -> \"Kneipe\"\n",
    "\n",
    "# (<bos>,\"ich\", \"bin\", \"ein\", <mask>) -> \"Junge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4693eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2991b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0 # Wir spalten die interne Darstellung \n",
    "                                                  # und dann wieder am Ende vereinigen \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch_size, T = Sequenzlänge, C = n_embd \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd, dim=2)\n",
    "        # n_head = nh, C = nh * hs, hs ist Head Size \n",
    "        # C= 768 , n_head = 12, hs = 64 \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q,k,v, is_causal=True) # flash attention / Effiziente Variante \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Die einzelnen Teilheads wieder vereinigen \n",
    "        # Ausgangsprojektion\n",
    "        y = self.c_proj(y) \n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4* config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config) # Causal Self-Attention = Masked Self-Attention \n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x)) # PreNorm \n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024      # Maximale Anzahl der Positionen = Länge vom Kontextfenster \n",
    "    vocab_size: int = 50257     # Anzahl der Tokens (gpt2-Tokenizer)\n",
    "    n_layer: int = 12           # Anzahl der Transformerblöcken \n",
    "    n_head: int = 12            # \n",
    "    n_embd: int = 768 \n",
    "\n",
    "\n",
    "class GPT(nn.Module): # Generative Pretrained Transformer 2 \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # Kontextfenster \n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx = (B, T) B ist batch_size, T ist Sequenzlänge\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size \n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) #   (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # FC \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2eb1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('goethe.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7cee5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 66404 tokens\n",
      "1 epoch = 129 batches\n",
      "step 0, loss: 10.998291015625, dt: 126.97 ms, tokens/sec: 4032.54\n",
      "step 1, loss: 9.023178100585938, dt: 62.42 ms, tokens/sec: 8202.73\n",
      "step 2, loss: 8.407600402832031, dt: 65.27 ms, tokens/sec: 7843.97\n",
      "step 3, loss: 8.326469421386719, dt: 63.95 ms, tokens/sec: 8005.86\n",
      "step 4, loss: 7.843833923339844, dt: 63.36 ms, tokens/sec: 8081.33\n",
      "step 5, loss: 7.40208625793457, dt: 66.56 ms, tokens/sec: 7692.44\n",
      "step 6, loss: 7.445466041564941, dt: 61.69 ms, tokens/sec: 8300.10\n",
      "step 7, loss: 7.678646087646484, dt: 64.53 ms, tokens/sec: 7934.19\n",
      "step 8, loss: 6.847314834594727, dt: 61.22 ms, tokens/sec: 8363.55\n",
      "step 9, loss: 6.927511215209961, dt: 66.82 ms, tokens/sec: 7662.50\n",
      "step 10, loss: 6.9487199783325195, dt: 63.35 ms, tokens/sec: 8081.72\n",
      "step 11, loss: 7.001776695251465, dt: 64.47 ms, tokens/sec: 7942.02\n",
      "step 12, loss: 6.70981502532959, dt: 62.29 ms, tokens/sec: 8220.00\n",
      "step 13, loss: 6.2171950340271, dt: 65.80 ms, tokens/sec: 7780.71\n",
      "step 14, loss: 6.191070556640625, dt: 65.94 ms, tokens/sec: 7764.76\n",
      "step 15, loss: 6.227861404418945, dt: 63.38 ms, tokens/sec: 8078.26\n",
      "step 16, loss: 6.246185302734375, dt: 64.97 ms, tokens/sec: 7880.33\n",
      "step 17, loss: 6.157041549682617, dt: 64.09 ms, tokens/sec: 7988.65\n",
      "step 18, loss: 6.195906639099121, dt: 65.28 ms, tokens/sec: 7843.03\n",
      "step 19, loss: 5.968327522277832, dt: 68.42 ms, tokens/sec: 7482.86\n",
      "step 20, loss: 6.01654052734375, dt: 62.17 ms, tokens/sec: 8235.48\n",
      "step 21, loss: 5.672636985778809, dt: 65.70 ms, tokens/sec: 7793.36\n",
      "step 22, loss: 5.673717498779297, dt: 61.01 ms, tokens/sec: 8392.31\n",
      "step 23, loss: 5.810541152954102, dt: 62.43 ms, tokens/sec: 8200.54\n",
      "step 24, loss: 5.685705184936523, dt: 62.61 ms, tokens/sec: 8177.43\n",
      "step 25, loss: 5.681684970855713, dt: 63.86 ms, tokens/sec: 8018.14\n",
      "step 26, loss: 5.73720121383667, dt: 60.05 ms, tokens/sec: 8526.09\n",
      "step 27, loss: 5.774383544921875, dt: 62.42 ms, tokens/sec: 8202.45\n",
      "step 28, loss: 5.649260520935059, dt: 61.72 ms, tokens/sec: 8295.93\n",
      "step 29, loss: 5.433812141418457, dt: 64.09 ms, tokens/sec: 7988.53\n",
      "step 30, loss: 5.603054046630859, dt: 63.27 ms, tokens/sec: 8092.81\n",
      "step 31, loss: 4.974170684814453, dt: 63.91 ms, tokens/sec: 8011.38\n",
      "step 32, loss: 5.269984722137451, dt: 61.25 ms, tokens/sec: 8358.80\n",
      "step 33, loss: 5.620944023132324, dt: 62.69 ms, tokens/sec: 8167.57\n",
      "step 34, loss: 5.173119068145752, dt: 65.71 ms, tokens/sec: 7791.80\n",
      "step 35, loss: 5.1414618492126465, dt: 62.55 ms, tokens/sec: 8185.07\n",
      "step 36, loss: 4.995227813720703, dt: 64.30 ms, tokens/sec: 7962.40\n",
      "step 37, loss: 5.226605415344238, dt: 64.43 ms, tokens/sec: 7947.08\n",
      "step 38, loss: 5.357505798339844, dt: 64.43 ms, tokens/sec: 7946.58\n",
      "step 39, loss: 5.86198616027832, dt: 66.39 ms, tokens/sec: 7712.47\n",
      "step 40, loss: 4.702946186065674, dt: 65.10 ms, tokens/sec: 7865.17\n",
      "step 41, loss: 5.077658176422119, dt: 59.67 ms, tokens/sec: 8580.80\n",
      "step 42, loss: 4.910313606262207, dt: 60.59 ms, tokens/sec: 8449.97\n",
      "step 43, loss: 5.274294853210449, dt: 62.43 ms, tokens/sec: 8201.45\n",
      "step 44, loss: 4.926226615905762, dt: 61.74 ms, tokens/sec: 8292.56\n",
      "step 45, loss: 5.103511810302734, dt: 65.52 ms, tokens/sec: 7815.00\n",
      "step 46, loss: 5.18147611618042, dt: 66.14 ms, tokens/sec: 7741.39\n",
      "step 47, loss: 4.879176139831543, dt: 61.46 ms, tokens/sec: 8330.36\n",
      "step 48, loss: 5.18998384475708, dt: 59.13 ms, tokens/sec: 8658.75\n",
      "step 49, loss: 5.060885429382324, dt: 64.10 ms, tokens/sec: 7988.14\n",
      "step 50, loss: 5.239831924438477, dt: 64.60 ms, tokens/sec: 7925.81\n",
      "step 51, loss: 4.903902530670166, dt: 63.78 ms, tokens/sec: 8028.22\n",
      "step 52, loss: 5.866386413574219, dt: 65.61 ms, tokens/sec: 7803.81\n",
      "step 53, loss: 5.699692249298096, dt: 62.97 ms, tokens/sec: 8131.24\n",
      "step 54, loss: 5.544205665588379, dt: 63.60 ms, tokens/sec: 8050.43\n",
      "step 55, loss: 5.406256675720215, dt: 65.88 ms, tokens/sec: 7771.81\n",
      "step 56, loss: 4.782395362854004, dt: 63.61 ms, tokens/sec: 8048.56\n",
      "step 57, loss: 4.954778671264648, dt: 64.51 ms, tokens/sec: 7936.27\n",
      "step 58, loss: 5.053442001342773, dt: 63.38 ms, tokens/sec: 8077.89\n",
      "step 59, loss: 4.987349510192871, dt: 63.75 ms, tokens/sec: 8031.37\n",
      "step 60, loss: 5.197811126708984, dt: 63.13 ms, tokens/sec: 8109.86\n",
      "step 61, loss: 5.45829963684082, dt: 61.73 ms, tokens/sec: 8294.71\n",
      "step 62, loss: 4.752676010131836, dt: 63.63 ms, tokens/sec: 8046.11\n",
      "step 63, loss: 4.409988880157471, dt: 65.37 ms, tokens/sec: 7832.93\n",
      "step 64, loss: 5.35725212097168, dt: 62.25 ms, tokens/sec: 8225.16\n",
      "step 65, loss: 5.138106346130371, dt: 63.19 ms, tokens/sec: 8102.27\n",
      "step 66, loss: 4.811187744140625, dt: 62.38 ms, tokens/sec: 8207.62\n",
      "step 67, loss: 4.967197895050049, dt: 62.72 ms, tokens/sec: 8162.73\n",
      "step 68, loss: 4.89937162399292, dt: 59.93 ms, tokens/sec: 8543.87\n",
      "step 69, loss: 4.967731475830078, dt: 64.19 ms, tokens/sec: 7976.84\n",
      "step 70, loss: 4.455013751983643, dt: 64.96 ms, tokens/sec: 7882.15\n",
      "step 71, loss: 4.47531270980835, dt: 60.80 ms, tokens/sec: 8421.04\n",
      "step 72, loss: 5.298289775848389, dt: 63.40 ms, tokens/sec: 8075.34\n",
      "step 73, loss: 4.708759307861328, dt: 62.10 ms, tokens/sec: 8245.06\n",
      "step 74, loss: 5.278556823730469, dt: 65.29 ms, tokens/sec: 7842.43\n",
      "step 75, loss: 4.753261566162109, dt: 61.92 ms, tokens/sec: 8268.46\n",
      "step 76, loss: 4.911979675292969, dt: 64.68 ms, tokens/sec: 7916.00\n",
      "step 77, loss: 5.305133819580078, dt: 62.77 ms, tokens/sec: 8156.65\n",
      "step 78, loss: 4.7249650955200195, dt: 62.20 ms, tokens/sec: 8232.04\n",
      "step 79, loss: 4.538991928100586, dt: 61.08 ms, tokens/sec: 8382.19\n",
      "step 80, loss: 5.102781772613525, dt: 62.77 ms, tokens/sec: 8157.33\n",
      "step 81, loss: 4.618454933166504, dt: 66.13 ms, tokens/sec: 7742.56\n",
      "step 82, loss: 4.299037933349609, dt: 61.81 ms, tokens/sec: 8283.29\n",
      "step 83, loss: 4.578516960144043, dt: 62.40 ms, tokens/sec: 8205.74\n",
      "step 84, loss: 4.76690149307251, dt: 63.84 ms, tokens/sec: 8019.79\n",
      "step 85, loss: 4.873603820800781, dt: 61.16 ms, tokens/sec: 8371.90\n",
      "step 86, loss: 4.66387939453125, dt: 62.91 ms, tokens/sec: 8138.85\n",
      "step 87, loss: 5.130858898162842, dt: 64.41 ms, tokens/sec: 7949.46\n",
      "step 88, loss: 4.3087592124938965, dt: 61.65 ms, tokens/sec: 8305.39\n",
      "step 89, loss: 5.300259590148926, dt: 63.96 ms, tokens/sec: 8005.38\n",
      "step 90, loss: 4.545145511627197, dt: 62.70 ms, tokens/sec: 8165.34\n",
      "step 91, loss: 4.818204402923584, dt: 63.97 ms, tokens/sec: 8004.37\n",
      "step 92, loss: 4.703359603881836, dt: 63.08 ms, tokens/sec: 8117.07\n",
      "step 93, loss: 4.189870357513428, dt: 65.75 ms, tokens/sec: 7787.57\n",
      "step 94, loss: 4.420415878295898, dt: 66.32 ms, tokens/sec: 7720.04\n",
      "step 95, loss: 4.270910263061523, dt: 62.19 ms, tokens/sec: 8232.99\n",
      "step 96, loss: 4.235764503479004, dt: 64.98 ms, tokens/sec: 7879.03\n",
      "step 97, loss: 4.631036758422852, dt: 67.57 ms, tokens/sec: 7576.88\n",
      "step 98, loss: 5.166727542877197, dt: 67.08 ms, tokens/sec: 7633.03\n",
      "step 99, loss: 5.061047077178955, dt: 65.09 ms, tokens/sec: 7866.53\n",
      "step 100, loss: 5.175047397613525, dt: 61.37 ms, tokens/sec: 8342.46\n",
      "step 101, loss: 4.280101776123047, dt: 63.03 ms, tokens/sec: 8123.24\n",
      "step 102, loss: 4.444606304168701, dt: 68.35 ms, tokens/sec: 7490.74\n",
      "step 103, loss: 4.65866756439209, dt: 67.85 ms, tokens/sec: 7546.26\n",
      "step 104, loss: 5.189868927001953, dt: 62.23 ms, tokens/sec: 8227.78\n",
      "step 105, loss: 5.976578712463379, dt: 66.74 ms, tokens/sec: 7671.58\n",
      "step 106, loss: 5.1644511222839355, dt: 67.64 ms, tokens/sec: 7569.93\n",
      "step 107, loss: 5.487727165222168, dt: 62.29 ms, tokens/sec: 8219.50\n",
      "step 108, loss: 5.1678972244262695, dt: 64.97 ms, tokens/sec: 7880.59\n",
      "step 109, loss: 5.20902681350708, dt: 62.72 ms, tokens/sec: 8162.76\n",
      "step 110, loss: 5.1368513107299805, dt: 66.70 ms, tokens/sec: 7676.63\n",
      "step 111, loss: 4.736654758453369, dt: 63.27 ms, tokens/sec: 8092.93\n",
      "step 112, loss: 4.730807304382324, dt: 63.01 ms, tokens/sec: 8125.94\n",
      "step 113, loss: 4.669831275939941, dt: 63.10 ms, tokens/sec: 8114.37\n",
      "step 114, loss: 4.786587238311768, dt: 63.69 ms, tokens/sec: 8039.16\n",
      "step 115, loss: 4.55084228515625, dt: 65.28 ms, tokens/sec: 7842.63\n",
      "step 116, loss: 4.490059852600098, dt: 62.12 ms, tokens/sec: 8242.21\n",
      "step 117, loss: 5.633072853088379, dt: 65.29 ms, tokens/sec: 7842.20\n",
      "step 118, loss: 5.340258598327637, dt: 63.62 ms, tokens/sec: 8048.04\n",
      "step 119, loss: 5.51796817779541, dt: 61.60 ms, tokens/sec: 8311.82\n",
      "step 120, loss: 5.384563446044922, dt: 59.93 ms, tokens/sec: 8543.63\n",
      "step 121, loss: 5.614871978759766, dt: 63.91 ms, tokens/sec: 8011.12\n",
      "step 122, loss: 6.183764457702637, dt: 65.77 ms, tokens/sec: 7784.24\n",
      "step 123, loss: 4.832083702087402, dt: 71.38 ms, tokens/sec: 7172.79\n",
      "step 124, loss: 4.7879838943481445, dt: 69.79 ms, tokens/sec: 7336.31\n",
      "step 125, loss: 4.485661506652832, dt: 68.37 ms, tokens/sec: 7488.34\n",
      "step 126, loss: 4.288465976715088, dt: 70.56 ms, tokens/sec: 7256.14\n",
      "step 127, loss: 4.285957336425781, dt: 65.58 ms, tokens/sec: 7807.02\n",
      "step 128, loss: 4.30204963684082, dt: 67.48 ms, tokens/sec: 7587.88\n",
      "step 129, loss: 5.889769554138184, dt: 66.60 ms, tokens/sec: 7687.46\n",
      "step 130, loss: 5.0552167892456055, dt: 71.19 ms, tokens/sec: 7192.23\n",
      "step 131, loss: 5.080257892608643, dt: 72.44 ms, tokens/sec: 7067.72\n",
      "step 132, loss: 5.08944034576416, dt: 71.05 ms, tokens/sec: 7205.98\n",
      "step 133, loss: 4.928276062011719, dt: 66.86 ms, tokens/sec: 7658.02\n",
      "step 134, loss: 4.738470077514648, dt: 72.17 ms, tokens/sec: 7094.38\n",
      "step 135, loss: 5.009137153625488, dt: 73.52 ms, tokens/sec: 6964.21\n",
      "step 136, loss: 4.695063591003418, dt: 71.51 ms, tokens/sec: 7159.35\n",
      "step 137, loss: 4.263759613037109, dt: 73.75 ms, tokens/sec: 6942.01\n",
      "step 138, loss: 4.754209041595459, dt: 69.09 ms, tokens/sec: 7410.10\n",
      "step 139, loss: 5.263552188873291, dt: 70.13 ms, tokens/sec: 7301.24\n",
      "step 140, loss: 5.300784111022949, dt: 73.53 ms, tokens/sec: 6963.06\n",
      "step 141, loss: 4.910916328430176, dt: 73.10 ms, tokens/sec: 7004.28\n",
      "step 142, loss: 4.5221734046936035, dt: 66.36 ms, tokens/sec: 7714.94\n",
      "step 143, loss: 4.713953971862793, dt: 71.15 ms, tokens/sec: 7195.74\n",
      "step 144, loss: 4.847539901733398, dt: 70.54 ms, tokens/sec: 7258.40\n",
      "step 145, loss: 5.060062885284424, dt: 69.09 ms, tokens/sec: 7410.38\n",
      "step 146, loss: 5.012291431427002, dt: 74.18 ms, tokens/sec: 6902.05\n",
      "step 147, loss: 5.168129920959473, dt: 73.92 ms, tokens/sec: 6926.14\n",
      "step 148, loss: 4.914322376251221, dt: 75.58 ms, tokens/sec: 6774.18\n",
      "step 149, loss: 4.896275997161865, dt: 70.10 ms, tokens/sec: 7303.97\n",
      "step 150, loss: 4.5640106201171875, dt: 73.40 ms, tokens/sec: 6975.27\n",
      "step 151, loss: 4.6892595291137695, dt: 73.83 ms, tokens/sec: 6934.48\n",
      "step 152, loss: 4.853357315063477, dt: 70.83 ms, tokens/sec: 7228.56\n",
      "step 153, loss: 4.750714302062988, dt: 69.72 ms, tokens/sec: 7343.53\n",
      "step 154, loss: 4.739131927490234, dt: 71.64 ms, tokens/sec: 7147.25\n",
      "step 155, loss: 4.876072406768799, dt: 71.11 ms, tokens/sec: 7200.52\n",
      "step 156, loss: 4.945554733276367, dt: 76.09 ms, tokens/sec: 6729.22\n",
      "step 157, loss: 4.6456804275512695, dt: 70.73 ms, tokens/sec: 7238.41\n",
      "step 158, loss: 4.479109764099121, dt: 72.93 ms, tokens/sec: 7020.81\n",
      "step 159, loss: 4.741004467010498, dt: 70.51 ms, tokens/sec: 7261.37\n",
      "step 160, loss: 4.1833882331848145, dt: 73.57 ms, tokens/sec: 6959.09\n",
      "step 161, loss: 4.301573753356934, dt: 73.21 ms, tokens/sec: 6993.76\n",
      "step 162, loss: 4.393022537231445, dt: 72.99 ms, tokens/sec: 7014.28\n",
      "step 163, loss: 4.129993438720703, dt: 76.67 ms, tokens/sec: 6677.71\n",
      "step 164, loss: 3.7673377990722656, dt: 74.29 ms, tokens/sec: 6892.30\n",
      "step 165, loss: 4.207207679748535, dt: 78.94 ms, tokens/sec: 6485.61\n",
      "step 166, loss: 4.249755382537842, dt: 73.68 ms, tokens/sec: 6949.36\n",
      "step 167, loss: 4.3980560302734375, dt: 78.52 ms, tokens/sec: 6521.02\n",
      "step 168, loss: 4.741456031799316, dt: 72.89 ms, tokens/sec: 7024.76\n",
      "step 169, loss: 3.8531267642974854, dt: 73.29 ms, tokens/sec: 6986.21\n",
      "step 170, loss: 4.30218505859375, dt: 73.07 ms, tokens/sec: 7006.79\n",
      "step 171, loss: 4.036188125610352, dt: 73.31 ms, tokens/sec: 6983.57\n",
      "step 172, loss: 4.373458385467529, dt: 75.73 ms, tokens/sec: 6761.11\n",
      "step 173, loss: 4.114747047424316, dt: 73.51 ms, tokens/sec: 6965.50\n",
      "step 174, loss: 4.274331092834473, dt: 74.62 ms, tokens/sec: 6861.06\n",
      "step 175, loss: 4.266228675842285, dt: 76.43 ms, tokens/sec: 6699.20\n",
      "step 176, loss: 3.9946157932281494, dt: 75.27 ms, tokens/sec: 6801.99\n",
      "step 177, loss: 4.396275520324707, dt: 75.51 ms, tokens/sec: 6780.73\n",
      "step 178, loss: 4.179859638214111, dt: 77.52 ms, tokens/sec: 6605.00\n",
      "step 179, loss: 4.432639122009277, dt: 77.54 ms, tokens/sec: 6603.11\n",
      "step 180, loss: 3.916339159011841, dt: 81.76 ms, tokens/sec: 6262.53\n",
      "step 181, loss: 4.802006721496582, dt: 83.13 ms, tokens/sec: 6159.11\n",
      "step 182, loss: 4.65579891204834, dt: 75.62 ms, tokens/sec: 6770.34\n",
      "step 183, loss: 4.547430515289307, dt: 84.01 ms, tokens/sec: 6094.81\n",
      "step 184, loss: 4.287113189697266, dt: 82.07 ms, tokens/sec: 6238.82\n",
      "step 185, loss: 3.7849326133728027, dt: 82.82 ms, tokens/sec: 6182.18\n",
      "step 186, loss: 3.9564695358276367, dt: 75.61 ms, tokens/sec: 6771.21\n",
      "step 187, loss: 4.071361541748047, dt: 83.92 ms, tokens/sec: 6100.98\n",
      "step 188, loss: 4.1134033203125, dt: 81.13 ms, tokens/sec: 6311.25\n",
      "step 189, loss: 4.04838752746582, dt: 84.07 ms, tokens/sec: 6089.89\n",
      "step 190, loss: 4.461223602294922, dt: 82.08 ms, tokens/sec: 6237.88\n",
      "step 191, loss: 4.067342758178711, dt: 84.06 ms, tokens/sec: 6091.10\n",
      "step 192, loss: 3.7052817344665527, dt: 82.89 ms, tokens/sec: 6177.00\n",
      "step 193, loss: 4.554163455963135, dt: 82.79 ms, tokens/sec: 6184.10\n",
      "step 194, loss: 4.276655197143555, dt: 84.11 ms, tokens/sec: 6087.13\n",
      "step 195, loss: 4.037681579589844, dt: 81.56 ms, tokens/sec: 6277.43\n",
      "step 196, loss: 4.130517482757568, dt: 83.95 ms, tokens/sec: 6098.59\n",
      "step 197, loss: 4.239355087280273, dt: 82.97 ms, tokens/sec: 6170.82\n",
      "step 198, loss: 4.201874732971191, dt: 82.17 ms, tokens/sec: 6231.36\n",
      "step 199, loss: 3.9198131561279297, dt: 81.78 ms, tokens/sec: 6260.48\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "train_loader = DataLoaderLite(B=8, T=64)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_rt_seqs = 1\n",
    "max_len  = 100\n",
    "\n",
    "model = GPT(GPTConfig)\n",
    "\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps= 10e-8)\n",
    "for i in range(200):\n",
    "    t0=time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y= x.to(device), y.to(device)\n",
    "    with torch.autocast(device_type=device, dtype = torch.bfloat16): # 16 bit floating point\n",
    "        logits,loss = model(x,y)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    dt = time.time() - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"step {i}, loss: {loss.item()}, dt: {dt*1000:.2f} ms, tokens/sec: {tokens_per_sec:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> FAUST Yet to see to me I think and not to know to the fire:\n",
      " \n",
      "To be your she:\n",
      "BR, and- is,\n",
      "As.\n",
      "MEPHISTOPHELES‘How the others.\n",
      " (And as there’s no lovely, in me.\n",
      "They do your child seems it A-ephist with theITCH\n",
      "The.)\n",
      "As that (To a little in your heart that\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "#tokens = enc.encode(\"How are you?\")\n",
    "tokens = enc.encode(\"FAUST\") # Prompt\n",
    "tokens = torch.tensor(tokens, dtype = torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_rt_seqs, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_len:\n",
    "    with torch.no_grad():\n",
    "        logits,loss = model(x)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        top_probs, top_ind = torch.topk(probs, 50, dim = -1)\n",
    "        ix = torch.multinomial(top_probs,1)\n",
    "        xcol = torch.gather(top_ind, -1, ix)\n",
    "        x = torch.cat((x,xcol), dim = 1)\n",
    "#\n",
    "for i in range(num_rt_seqs):\n",
    "    tokens = x[i, :max_len].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
